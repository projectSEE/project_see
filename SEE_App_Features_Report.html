<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>SEE App - Features & Techniques Report</title>
<style>
  body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    max-width: 900px;
    margin: 40px auto;
    padding: 20px 40px;
    color: #333;
    line-height: 1.6;
    background: #fff;
  }
  h1 {
    color: #1a73e8;
    border-bottom: 3px solid #1a73e8;
    padding-bottom: 10px;
    font-size: 28px;
  }
  h2 {
    color: #1a73e8;
    margin-top: 30px;
    font-size: 22px;
  }
  h3 {
    color: #333;
    margin-top: 25px;
    font-size: 18px;
  }
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 15px 0;
    font-size: 14px;
  }
  th {
    background: #1a73e8;
    color: white;
    padding: 10px 15px;
    text-align: left;
  }
  td {
    padding: 10px 15px;
    border-bottom: 1px solid #e0e0e0;
  }
  tr:nth-child(even) td {
    background: #f8f9fa;
  }
  blockquote {
    background: #e8f0fe;
    border-left: 4px solid #1a73e8;
    padding: 12px 20px;
    margin: 15px 0;
    border-radius: 0 8px 8px 0;
    font-style: italic;
  }
  pre {
    background: #f5f5f5;
    padding: 15px;
    border-radius: 8px;
    overflow-x: auto;
    font-size: 13px;
  }
  code.inline {
    background: #f0f0f0;
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 13px;
  }
  hr {
    border: none;
    border-top: 2px solid #e0e0e0;
    margin: 30px 0;
  }
  ul, ol {
    padding-left: 25px;
  }
  li {
    margin: 5px 0;
  }
  @media print {
    body { margin: 0; padding: 20px; }
    h2 { page-break-before: auto; }
    table { page-break-inside: avoid; }
  }
</style>
</head>
<body>
<h1>SEE App ‚Äî Features & Techniques Report</h1>
<br>
<blockquote>**SEE** (Smart Eye Experience) is a Flutter-based mobile application designed to assist blind and visually impaired users in navigating the world safely and independently.</blockquote>
<br>
<hr>
<br>
<h2>Architecture Overview</h2>
<br>
<pre><code class="mermaid">
graph TD
    A[Camera Feed] --&gt; B[ML Kit Object Detection]
    A --&gt; C[Depth Anything V2 - ONNX]
    A --&gt; D[ML Kit Text Recognition]
    A --&gt; E[ML Kit Image Labeling]
    A --&gt; F[Gemini Live API]

    B --&gt; G[Context Aggregator]
    C --&gt; G
    D --&gt; G
    E --&gt; G

    G --&gt; H[Text-to-Speech]
    G --&gt; I[Haptic Vibration]

    F --&gt; J[Real-time Audio Response]

    K[GPS / Geolocator] --&gt; L[Navigation Service]
    L --&gt; H

    M[Accelerometer] --&gt; N[Fall Detection]
    N --&gt; O[Emergency SOS]
</code></pre>
<br>
<hr>
<br>
<h2>Core Features for Blind Users</h2>
<br>
<h3>1. üîç Real-Time Object Detection</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Google ML Kit Object Detection (on-device)</td></tr>
<tr><td>**How it helps**</td><td>Detects and identifies objects in the camera feed in real-time, telling users what's around them</td></tr>
<tr><td>**Key characteristic**</td><td>Runs entirely on-device ‚Äî no internet required, low latency</td></tr>
<tr><td>**Output**</td><td>Object label, position (left/center/right), relative size (distance estimate)</td></tr>
</table>
<br>
<hr>
<br>
<h3>2. üìè Depth Estimation (Distance Measurement)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Depth Anything V2 model via ONNX Runtime</td></tr>
<tr><td>**How it helps**</td><td>Estimates how far objects are from the user, critical for safe navigation</td></tr>
<tr><td>**Key characteristics**</td><td>Temporal trend tracking (detects if objects are approaching or receding), categorizes distances as "very close / close / nearby / far"</td></tr>
<tr><td>**Unique feature**</td><td>`DepthChangeResult` tracks whether objects are getting closer ‚Äî triggers stronger vibration warnings for approaching hazards</td></tr>
</table>
<br>
<hr>
<br>
<h3>3. üì≥ Proximity-Based Haptic Feedback</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Vibration plugin + Flutter HapticFeedback (fallback)</td></tr>
<tr><td>**How it helps**</td><td>Provides tactile warnings without requiring audio ‚Äî users can feel danger through their phone</td></tr>
<tr><td>**Intensity levels**</td><td>**Strong** (>10% proximity) ‚Üí **Medium** (>5%) ‚Üí **Light** (>1%)</td></tr>
<tr><td>**Special patterns**</td><td>Emergency warning pattern (rapid triple vibration), intensity boost for approaching objects</td></tr>
<tr><td>**Key characteristic**</td><td>300ms throttle to prevent overwhelming feedback while remaining responsive</td></tr>
</table>
<br>
<hr>
<br>
<h3>4. üó£Ô∏è Text-to-Speech (TTS) Announcements</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>`flutter_tts`</td></tr>
<tr><td>**How it helps**</td><td>Converts all visual information into spoken words ‚Äî the primary output channel for blind users</td></tr>
<tr><td>**Used by**</td><td>Obstacle announcements, navigation instructions, scene descriptions, text reading</td></tr>
</table>
<br>
<hr>
<br>
<h3>5. üìñ Text Recognition (OCR)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Google ML Kit Text Recognition</td></tr>
<tr><td>**How it helps**</td><td>Reads signs, labels, room numbers, and important text in the environment</td></tr>
<tr><td>**Smart filtering**</td><td>Prioritizes important text containing keywords: "exit", "danger", "warning", "toilet", "push", "pull", floor/room numbers</td></tr>
<tr><td>**Key characteristic**</td><td>Ignores noise (very short text) and focuses on navigation-relevant information</td></tr>
</table>
<br>
<hr>
<br>
<h3>6. üè∑Ô∏è Image Labeling (Scene Understanding)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Google ML Kit Image Labeling</td></tr>
<tr><td>**How it helps**</td><td>Identifies the broader scene context (e.g., "indoor", "street", "park") to give users environmental awareness</td></tr>
<tr><td>**Key characteristic**</td><td>Complements object detection with higher-level scene understanding</td></tr>
</table>
<br>
<hr>
<br>
<h3>7. ü§ñ Gemini Live AI Assistant</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Firebase AI / Gemini 2.0 Flash Live Preview (Vertex AI)</td></tr>
<tr><td>**How it helps**</td><td>Real-time conversational AI that acts as a **virtual pair of eyes** ‚Äî users can ask questions about their surroundings</td></tr>
<tr><td>**Capabilities**</td><td>Receives live camera frames + audio ‚Üí responds with spoken audio descriptions</td></tr>
<tr><td>**System prompt**</td><td>Configured specifically for blind users: uses directional language ("ahead, left, right"), estimates distances, prioritizes hazards</td></tr>
<tr><td>**Voice output**</td><td>Direct audio response using "Puck" voice ‚Äî Gemini speaks the descriptions itself</td></tr>
<tr><td>**Key characteristic**</td><td>Bidirectional: users can speak questions, and receive spoken answers about what the camera sees</td></tr>
</table>
<br>
<hr>
<br>
<h3>8. üß† Context Aggregator (Smart Announcement System)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Custom service combining all ML Kit outputs</td></tr>
<tr><td>**How it helps**</td><td>Prevents information overload by intelligently combining and prioritizing announcements</td></tr>
<tr><td>**Priority system**</td><td>**High** (vibrate + speak immediately for close obstacles) ‚Üí **Medium** (speak when convenient for text) ‚Üí **Low** (optional info)</td></tr>
<tr><td>**Key characteristic**</td><td>3-second update interval, only announces significant changes to reduce repetitive output</td></tr>
</table>
<br>
<hr>
<br>
<h3>9. üó∫Ô∏è Walking Navigation</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Google Maps API + Geolocator + Geocoding</td></tr>
<tr><td>**How it helps**</td><td>Provides step-by-step walking directions optimized for blind users</td></tr>
<tr><td>**Voice instructions**</td><td>Converts route steps into blind-friendly language (e.g., "Walk ahead for 50 meters, then turn right")</td></tr>
<tr><td>**Key characteristics**</td><td>Real-time GPS tracking, automatic step advancement when user reaches waypoint, repeat/skip instruction controls</td></tr>
<tr><td>**Place search**</td><td>Search for nearby places (restaurants, hospitals, etc.) with distance information</td></tr>
</table>
<br>
<hr>
<br>
<h3>10. üö® Fall Detection & Emergency SOS</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>`sensors_plus` (accelerometer)</td></tr>
<tr><td>**How it helps**</td><td>Detects if the user falls ‚Äî critical safety feature for blind users who may trip on unseen obstacles</td></tr>
<tr><td>**Detection method**</td><td>Two-stage: detects **free-fall** (low acceleration < 1.5g) followed by **impact** (high acceleration > 20g) within 500ms</td></tr>
<tr><td>**Emergency response**</td><td>15-second countdown with alarm siren ‚Üí auto-calls emergency contact if not cancelled</td></tr>
<tr><td>**Key characteristic**</td><td>User can cancel during countdown to prevent false alarms</td></tr>
</table>
<br>
<hr>
<br>
<h3>11. üí¨ AI Chat with Multimodal Input</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>Firebase AI (Gemini) + Cloud Firestore + TTS</td></tr>
<tr><td>**How it helps**</td><td>Users can have conversations with AI, including sending photos for identification</td></tr>
<tr><td>**Capabilities**</td><td>Text input, voice input (speech-to-text), image input (camera/gallery), live mode (real-time audio conversation)</td></tr>
<tr><td>**Data persistence**</td><td>Chat history stored in Firestore, organized by topics</td></tr>
<tr><td>**Export**</td><td>Conversations can be exported as PDF</td></tr>
</table>
<br>
<hr>
<br>
<h3>12. üé§ Voice Input (Speech-to-Text)</h3>
<table>
<tr><th>Aspect</th><th>Details</th></tr>
<tr><td>**Technology**</td><td>`speech_to_text` + `record` (PCM audio)</td></tr>
<tr><td>**How it helps**</td><td>Blind users can interact hands-free ‚Äî speak commands and questions instead of typing</td></tr>
<tr><td>**Key characteristic**</td><td>Works in both chat mode and obstacle detection mode</td></tr>
</table>
<br>
<hr>
<br>
<h2>Technology Stack Summary</h2>
<br>
<table>
<tr><th>Category</th><th>Technologies</th></tr>
<tr><td>**Framework**</td><td>Flutter 3.27.0 (Dart)</td></tr>
<tr><td>**On-Device ML**</td><td>Google ML Kit (Object Detection, Text Recognition, Image Labeling)</td></tr>
<tr><td>**Depth Estimation**</td><td>Depth Anything V2 via ONNX Runtime</td></tr>
<tr><td>**AI/LLM**</td><td>Firebase AI / Gemini 2.0 Flash (Live API for real-time, standard for chat)</td></tr>
<tr><td>**Navigation**</td><td>Google Maps Flutter, Geolocator, Geocoding, Google Routes API</td></tr>
<tr><td>**Audio**</td><td>flutter_tts, speech_to_text, just_audio, record, flutter_pcm_sound</td></tr>
<tr><td>**Haptics**</td><td>Vibration plugin + Flutter HapticFeedback</td></tr>
<tr><td>**Sensors**</td><td>sensors_plus (accelerometer for fall detection)</td></tr>
<tr><td>**Backend**</td><td>Firebase (Core, Firestore, Storage)</td></tr>
<tr><td>**Platform**</td><td>Android (primary), iOS (supported)</td></tr>
</table>
<br>
<hr>
<br>
<h2>What Makes This App Genuinely Useful for Blind Users</h2>
<br>
<ol>
<li><strong>Multi-sensory output</strong> ‚Äî Information is delivered through both <strong>voice (TTS)</strong> and <strong>touch (vibration)</strong>, so users always receive feedback regardless of environment noise</li>
<li><strong>Prioritized information</strong> ‚Äî The context aggregator prevents information overload by only announcing significant, safety-relevant changes</li>
<li><strong>Approaching object detection</strong> ‚Äî Depth trend tracking warns users about objects getting closer, not just static obstacles</li>
<li><strong>Hands-free operation</strong> ‚Äî Voice input + automatic detection means minimal physical interaction needed</li>
<li><strong>Safety net</strong> ‚Äî Fall detection with auto-emergency-call provides a safety backup that sighted users take for granted</li>
<li><strong>Real-time AI assistant</strong> ‚Äî Gemini Live allows natural conversation about surroundings, going beyond simple detection to contextual understanding</li>
<li><strong>Offline-capable core</strong> ‚Äî Object detection and depth estimation run on-device, working without internet</li>
<li><strong>Walking-optimized navigation</strong> ‚Äî Route instructions are converted into blind-friendly directional language</li>
</ul>
<br>
</body>
</html>